# <font face="Cambria Math" color=black size=6>主要内容概括</font>

这篇文章的主要内容是：
- **大型语言模型的自动思维链提示**：这篇文章提出了一种方法，可以利用大型语言模型（LLMs）的多步推理能力，通过自动生成示例来提示LLMs产生中间推理步骤。这种方法叫做Auto-CoT，它通过对问题进行聚类和采样，以及使用零样本推理生成推理链，来构造示例。实验结果表明，Auto-CoT在十个公开的推理任务数据集上，都能达到或超过手工设计示例的效果。
- **思维链提示的两种范式**：思维链提示（CoT prompting）是一种梯度无关的技术，可以引导LLMs产生一系列连贯的中间推理步骤，从而得到问题的最终答案。思维链提示有两种主要的范式：一种是使用一个简单的提示语，如“让我们一步一步地思考”，来促进LLMs进行分步思考（Zero-Shot-CoT）；另一种是使用少量手工编写的示例，每个示例包含一个问题和一个推理链（Manual-CoT）。实践中，Manual-CoT有更强的性能，但是需要人工设计任务相关的示例。
- **自动思维链提示的挑战和方法**：为了消除人工设计示例的需求，作者提出了Auto-CoT方法来自动构造示例。Auto-CoT包括两个主要步骤：（1）问题聚类：将给定数据集的问题分成几个簇；（2）示例采样：从每个簇中选择一个代表性的问题，并使用Zero-Shot-CoT和简单的启发式规则生成其推理链。作者发现，采样问题时，多样性比相似性更重要，因为多样性可以减轻由于零样本推理错误导致的误导效应。


**代码链接：https://github.com/amazon-research/auto-cot**


# 文章局限性

- **依赖于LLMs的零样本推理能力**：这篇文章的方法使用零样本推理来生成示例中的推理链，但是LLMs的零样本推理能力并不完美，可能会产生错误或不完整的推理链。这些错误的推理链可能会误导LLMs在测试问题上进行错误或不充分的推理。
- **缺乏对示例质量的评估和优化**：这篇文章的方法没有提供一个有效的指标或方法来评估和优化自动生成的示例的质量。例如，如何判断一个示例是否能够有效地引导LLMs进行推理？如何避免生成冗余或无关的示例？如何根据不同任务或问题类型调整示例的数量和格式？
- **忽略了任务相关的信息和指导**：这篇文章的方法没有利用任何任务相关的信息和指导来构造示例。例如，对于算术推理任务，可能需要提供一些数学符号或公式的解释；对于常识推理任务，可能需要提供一些常识知识或规则的提示。这些信息和指导可能会帮助LLMs更好地理解问题和答案。


# 文章主要贡献

- **提出了一种自动思维链提示的方法**：这篇文章提出了一种利用大型语言模型（LLMs）进行多步推理的方法，可以通过自动生成示例来提示LLMs产生中间推理步骤。这种方法叫做Auto-CoT，它通过对问题进行聚类和采样，以及使用零样本推理生成推理链，来构造示例。这种方法消除了人工设计示例的需求，并且在十个公开的推理任务数据集上都能达到或超过手工设计示例的效果。
- **发现了多样性对于自动思维链提示的重要性**：这篇文章发现了多样性是自动构造示例的关键因素，因为多样性可以减轻由于零样本推理错误导致的误导效应。这篇文章提出了一种基于聚类和启发式规则的多样性采样方法，可以有效地选择不同类型和难度的问题，并生成合适的推理链作为示例。
- **分析了自动思维链提示的挑战和局限性**：这篇文章分析了自动思维链提示面临的一些挑战和局限性，例如依赖于LLMs的零样本推理能力、缺乏对示例质量的评估和优化、忽略了任务相关的信息和指导等。这篇文章也提出了一些可能的解决方案或未来的研究方向，例如使用更强大或更专业化的LLMs、设计更有效的指标或方法来评估和优化示例质量、利用任务相关的信息和指导来增强LLMs的推理能力等。

# 贡献重要吗？为什么？
这篇文章的贡献是非常重要的。它提出了一种新颖的方法，可以利用大型语言模型（LLMs）的多步推理能力，通过自动生成示例来提示LLMs产生中间推理步骤。这种方法消除了人工设计示例的需求，并且在十个公开的推理任务数据集上都能达到或超过手工设计示例的效果。

此外，这篇文章还发现了多样性对于自动思维链提示的重要性，并提出了一种基于聚类和启发式规则的多样性采样方法。这些贡献为自然语言处理领域提供了新的思路和方法，可以帮助研究人员更好地理解和利用LLMs的推理能力。

这篇文章为自然语言处理领域提供了新的思路和方法，并为未来的研究提供了巨大的挑战和机会。


# 这篇文章的研究假设是什么，这些假设是否合理、局限或者过于简化？

研究假设有以下方面：

- **大型语言模型（LLMs）具有多步推理能力**：这篇文章假设LLMs可以通过自动生成示例来进行多步推理，从而解决算术、常识和符号推理任务。这些示例可以作为一种新的知识表示形式，可以帮助人类和机器更好地理解和交流问题和答案。
- **示例的质量和多样性对于多步推理的影响**：这篇文章假设示例的质量和多样性对于多步推理的影响是显著的，发现了一些有用的启发式规则和指标，可以用来评估和优化示例的质量和多样性。这些规则和指标可以帮助提高示例的有效性和覆盖性，从而提高LLMs在不同任务上的泛化能力。
- **自动思维链提示的挑战和局限性**：这篇文章假设自动思维链提示面临的一些挑战和局限性是可克服的，例如依赖于LLMs的零样本推理能力、缺乏对示例质量的评估和优化、忽略了任务相关的信息和指导等。这篇文章也提出了一些可能的解决方案或未来的研究方向，例如使用更强大或更专业化的LLMs、设计更有效的指标或方法来评估和优化示例质量、利用任务相关的信息和指导来增强LLMs的推理能力等。

我认为这些假设是合理但也有局限性或过于简化的。

合理之处在于，这些假设都是基于现有的研究成果或经验进行推断或验证的，例如LLMs已经表现出了一定的零样本推理能力 [Kojima et al., 2022]，示例对于引导LLMs进行推理是有效的 [Wei et al., 2022a]，任务相关的信息和指导可以增强LLMs的推理能力 [Mishra et al., 2022, Wei et al., 2022b, Sanh et al., 2022]等。

局限或过于简化之处在于，这些假设可能忽略了一些重要的因素或问题，例如LLMs是否真正理解了问题和答案背后的逻辑和常识 [Bender and Koller, 2020]，示例是否存在一些潜在的偏见或误导 [Liu et al., 2022b, Min et al., 2022b]，任务相关的信息和指导是否足够清晰和完整 [Webson and Pavlick, 2022, Zhao et al., 2021]等。

# 基于这篇论文的可能应用有哪些？

- **自然语言处理**：这篇论文提出了一种利用大型语言模型（LLMs）进行多步推理的方法，可以提高自然语言处理模型在常识推理任务上的性能。这些任务包括问答、阅读理解、事件续写等，都需要模型具备一定的常识和逻辑推理能力。
- **知识获取和表示**：这篇论文展示了如何通过自动生成示例来提示LLMs产生中间推理步骤，从而获取和表示知识。这些示例可以作为一种新的知识表示形式，可以帮助人类和机器更好地理解和交流问题和答案。
- **教育和娱乐**：这篇论文也可以为教育和娱乐领域提供一些启示，例如如何设计更有趣和有挑战性的问题，以及如何生成更有创意和有逻辑性的答案。

# 在该论文的基础上，有哪些工作可以继续延伸下去？如何延申？

- **改进示例的质量和多样性**：这篇论文使用了一些简单的启发式规则和指标来评估和优化示例的质量和多样性，但是这些规则和指标可能不够有效或通用。可以尝试设计更有效的方法来评估和优化示例的质量和多样性，例如使用强化学习、元学习、对抗学习等技术，让语言模型自动学习如何生成更好的示例。
- **利用任务相关的信息和指导**：这篇论文没有利用任何任务相关的信息和指导来构造示例，例如任务说明、知识库、常识规则等。可以尝试利用这些信息和指导来增强语言模型的推理能力，例如使用知识图谱、逻辑推理、符号操作等技术，让语言模型能够处理更复杂和更抽象的推理问题。
- **扩展到其他类型的推理任务**：这篇论文主要关注了算术、常识和符号推理任务，但是还有其他类型的推理任务，例如因果推理、类比推理、空间推理等。可以尝试将自动思维链提示的方法扩展到其他类型的推理任务，探索语言模型在不同领域和场景下的推理能力和局限性。



# 和之前读到的论文之间有哪些关系？
- Language Models are Few-Shot Learners [Brown et al., 2020]：这篇论文是GPT-3模型的原始论文，它展示了GPT-3在零样本或少样本条件下，在各种自然语言处理任务上取得了令人印象深刻的结果。这篇论文也探讨了GPT-3在常识推理方面的能力和局限性。
- LAMDA: Language Models for Dialog Applications [Thoppilan et al., 2022]：这篇论文介绍了LAMDA模型，它是一个基于GPT-3扩展而成的大型语言模型，专门用于对话应用。这篇论文展示了LAMDA模型在各种对话场景下，如问答、任务完成、闲聊等，都能表现出强大的语言理解和生成能力。
- Scaling Language Models: Methods, Analysis & Insights from Training Gopher [Rae et al., 2021]：这篇论文介绍了Gopher模型，它是一个拥有1.6万亿参数的大型语言模型，是目前最大规模的语言模型之一。这篇论文描述了训练Gopher模型所使用的方法、分析和洞察，以及Gopher模型在各种任务上的表现。
- PALM: Scaling Language Modeling with Pathways [Chowdhery et al., 2022]：这篇论文介绍了PALM模型，它是一个基于路径结构（pathways）来组织参数和计算流图的大型语言模型。这篇论文展示了PALM模型在各种任务上，如问答、摘要、翻译等，都能达到最先进或接近最先进的结果。
- Chain of Thought Prompting Elicits Reasoning in Large Language Models [Wei et al., 2022a]：这篇论文提出了思维链提示（CoT prompting）的概念，它是一种利用大型语言模型进行多步推理的技术，通过手工设计示例来提示语言模型产生中间推理步骤。这篇论文展示了CoT prompting在十个公开的推理任务数据集上，都能显著提高语言模型的性能。
- Large Language Models are Zero-Shot Reasoners [Kojima et al., 2022]：这篇论文探讨了大型语言模型的零样本推理能力，它是指在没有针对特定任务进行训练的情况下，利用模型的先验知识和推理能力来解决问题。这篇论文展示了GPT-3模型在各种推理任务上，如算术、常识、符号等，都能表现出一定的零样本推理能力。





  

# 项目构想
可以基于这篇论文的思想，设计一个自动思维链提示的系统，可以帮助用户解决各种推理问题，如算术、常识、符号等。这个系统可以接收用户输入的问题，然后根据问题的类型和难度，自动构造合适的示例，并展示给用户看。用户可以通过查看示例中的推理步骤，来理解和学习如何解决问题。

# 具体应用
增强大模型的中文推理能力（高考题为数据集）

推理能力(任务一)    

初步思路：利用Auto-CoT,先将进行问题聚类（想法：按照学科进行聚类）,然后示例采样（每个学科问题簇中选择一个代表性问题），使用Zero-Shot-CoT（"让我们一步一步地思考"）和简单的启发式规则生成其推理链（保障采样的多样性）

创新思路：评价示例的质量和多样性，需要考虑许多因素，如示例的准确性、可靠性、相关性、代表性等。此外，不同的任务或问题类型可能需要不同的评价标准和方法。









# 思考


### 1-Auto-CoT和Manual-CoT的区别
- Auto-CoT和Manual-CoT是两种不同的思维链提示范式。它们都可以引导大型语言模型（LLMs）产生一系列连贯的中间推理步骤，从而得到问题的最终答案。

- Manual-CoT使用少量手工编写的示例，每个示例包含一个问题和一个推理链。这种方法需要人工设计任务相关的示例，但是实践中它有更强的性能。

- 相比之下，Auto-CoT是一种自动构造示例的方法。它通过对问题进行聚类和采样，以及使用零样本推理生成推理链，来构造示例。这种方法消除了人工设计示例的需求，并且在十个公开的推理任务数据集上都能达到或超过手工设计示例的效果。

- Auto-CoT和Manual-CoT都是思维链提示范式，但它们在构造示例方面有所不同。Manual-CoT需要人工设计任务相关的示例，而Auto-CoT则可以自动构造示例。




### 2-数据集

- **BoolQ**：一个布尔型问题回答数据集，包含2,760个问题和答案，以及与问题相关的维基百科段落。
- **COPA**：一个因果推理数据集，包含1,000个问题和答案，每个问题要求从两个选项中选择一个正确的因果关系。
- **HellaSwag**：一个常识推理数据集，包含39,905个问题和答案，每个问题要求从四个选项中选择一个正确的事件续写。
- **LAMA**：一个知识库问答数据集，包含43,757个问题和答案，每个问题要求从知识库中填充一个缺失的实体或属性。
- **MC-TACO**：一个时序常识推理数据集，包含7,762个问题和答案，每个问题要求从四个选项中选择一个正确的时间跨度。
- **MultiRC**：一个多选阅读理解数据集，包含5,467个段落、21,142个问题和48,579个答案，每个问题要求从多个选项中选择所有正确的答案。
- **PIQA**：一个物理常识推理数据集，包含16,113个问题和答案，每个问题要求从两个选项中选择一个正确的物理操作。
- **ReCoRD**：一个阅读理解数据集，包含10,101篇新闻文章、120,591个问题和302,932个答案，每个问题要求从文章中提取一个或多个实体作为答案。
- **SocialIQA**：一个社会常识推理数据集，包含37,905个问题和答案，每个问题要求从三个选项中选择一个正确的社会行为。
- **WSC**：一个代词消歧数据集，包含273个句子和代词，每个句子要求从两个候选名词中选择一个正确的代词指代。

**PS.数据集补充说明**
- BoolQ数据集包含了一些自然产生的是/非问题，这些问题需要复杂的非事实性信息和类似于蕴含的推理来解决。
- HellaSwag数据集则是一个新的挑战性数据集，它的问题对人类来说非常简单（>95%的准确率），但对于最先进的模型来说却很困难（<48%）。


```
(HellaSwag介绍)
HellaSwag是一个常识推理数据集，它包含了39,905个问题和答案，每个问题要求从四个选项中选择一个正确的事件续写。

eg.
问题：在一家餐厅，一位顾客点了一杯咖啡。然后，服务员...

选项：
A. 把咖啡倒在了顾客的头上。
B. 把咖啡端到了顾客的桌子上。
C. 把咖啡倒进了自己的口袋里。
D. 把咖啡倒进了鱼缸里。

正确答案：B. 把咖啡端到了顾客的桌子上。

这个例子展示了HellaSwag数据集中的一个典型问题。它要求模型根据常识推理出最可能发生的事件续写。
```


### 3-Auto-CoT的表现

- **算术推理**：Auto-CoT在MultiArith, GSM8K, AddSub, AQuA, SingleEq和SVAMP这六个算术推理数据集上，都超过了手工设计示例的Manual-CoT的准确率，达到了84.8%到92.0%的范围。
- **常识推理**：Auto-CoT在CSQA和StrategyQA这两个常识推理数据集上，与Manual-CoT的准确率相当，分别为74.4%和65.4%。
- **符号推理**：Auto-CoT在Last Letter Concatenation和Coin Flip这两个符号推理数据集上，也与Manual-CoT的准确率相当，分别为59.7%和99.9%。

![image](https://github.com/leejamesss/paper-reading/assets/117844938/856c3acb-355d-43b6-8fd9-e88ad78ad621)


![image](https://github.com/leejamesss/paper-reading/assets/117844938/9901fec4-de9b-48e4-bc33-0f91be5398b4)


Auto-CoT在十个公开的推理任务数据集上，都能达到或超过手工设计示例的Manual-CoT的表现。这说明Auto-CoT可以利用大型语言模型（LLMs）的多步推理能力，通过自动生成示例来提示LLMs产生中间推理步骤。


### 4-LLM的零样本推理
零样本推理（Zero-Shot Reasoning）是指在没有针对特定任务进行训练的情况下，利用模型的先验知识和推理能力来解决问题。大型语言模型（LLMs）在训练过程中学习了大量的语言知识和常识，因此它们具有一定的零样本推理能力。

例如，如果给LLMs一个问题，它可以根据自己学到的知识和推理规则，来生成一个合理的答案，而不需要额外的训练数据。这种能力对于处理新颖或未见过的问题非常有用，因为它可以让模型在没有针对特定任务进行训练的情况下，仍然能够给出合理的答案。

当然，LLMs的零样本推理能力并不完美，它们可能会产生错误或不完整的推理结果。因此，在实际应用中，通常需要结合其他技术（如迁移学习、元学习等）来提高模型在特定任务上的性能。

### 5-Auto-CoT流程
- 问题聚类：将给定数据集的问题分成几个簇，每个簇包含相似的问题。
- 示例采样：从每个簇中选择一个代表性的问题，并使用零样本推理生成其推理链，作为示例。
- 示例筛选：根据一些启发式规则，如问题和推理链的长度、复杂度等，筛选出符合条件的示例。
- 上下文推理：将所有选出的示例拼接在一起，作为上下文信息，输入到语言模型中，然后在后面加上测试问题和提示语，让语言模型输出测试问题的推理链和答案。

### 6-Auto-CoT启发式规则解析
![image](https://github.com/leejamesss/paper-reading/assets/117844938/ff02c6f4-f9c8-4f0d-b8af-51769d39d3b1)




Auto-CoT是一种自动化的链式思维提示方法，它采用了一些启发式规则来构建演示。在第二阶段，我们需要为那些采样问题生成推理链，然后采样满足我们选择标准的演示。具体来说，我们构造一个演示d(i)（问题、理由和答案的连接）用于每个集群i（i = 1，...，k）。对于集群i，我们迭代q(i) = [q(i)1, q(i)2, ...]中的问题（由算法1获得），直到满足我们的选择标准。换句话说，距离集群i中心更近的问题将被更早考虑。假设第j个最近的问题q(i)j正在被考虑。提示输入被格式化为：[Q: q(i)j. A: [P]]，其中[P]是一个单独的提示“让我们一步一步地思考”。这个形成的输入被馈送到LLM使用Zero-Shot-CoT [Kojima et al., 2022]输出包含理由r(i)j和提取答案a(i)j的推理链。¹[1]然后，为第i个集群构造一个候选演示d(i)j，通过连接问题、理由和答案：[Q: q(i)j, A: r(i)j◦a(i)j]。

类似于Wei et al. [2022a]中手工制作演示的标准，我们的选择标准遵循简单的启发式规则来鼓励采样更简单的问题和理由：如果q(i)j、r(i)j满足选择标准，则将所选演示d(i)设置为d(i)j。这些选择标准是鼓励采样更简单的问题和理由：设置所选演示d(i)为d(i)j，如果它有一个不超过60个令牌的问题q(i)j和不超过5个推理步骤的理由r(i)j。





