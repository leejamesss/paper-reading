
# 主要内容

- **LLaMA-Adapter**：一种高效的适配方法，可以将LLaMA转化为一个能够根据指令生成响应的语言模型。只需要1.2M个可学习的参数和一个小时的训练，就可以有效地微调LLaMA，并且与全微调的7B参数的Alpaca相比具有优越的效率。
- **零初始化注意力**：为了提高训练的稳定性和最终的性能，我们提出了一种零初始化注意力机制，可以自适应地将指令信号注入到LLaMA中，同时有效地保留了LLaMA的预训练生成知识。
- **多模态推理**：我们的方法不仅限于文本指令，还可以扩展到图像条件下的多模态推理，在ScienceQA基准上表现出竞争性的性能。
