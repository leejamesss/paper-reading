![image](https://github.com/leejamesss/paper-reading/assets/117844938/123eca27-396c-4e59-8b14-33ce99497d94)


![image](https://github.com/leejamesss/paper-reading/assets/117844938/22342b6e-8839-47f6-9b7e-2fdd8b70655f)

# 具体内容
这篇论文的第一部分讲述了：

- **前缀调优**：一种轻量级的替代微调的方法，用于自然语言生成任务。它保持语言模型参数不变，但优化一个小的连续任务特定向量（称为前缀），作为输入的一部分。前缀调优受到提示的启发，允许后续的令牌像“虚拟令牌”一样关注这个前缀。
- **实验结果**：在表格到文本生成和摘要生成两个任务上，使用GPT-2和BART作为预训练的语言模型，前缀调优只需要0.1%的任务特定参数，就能达到与微调相当或更好的性能。在低数据设置和外推设置下，前缀调优也优于微调。
- **内部评估**：对前缀调优的不同变体进行了比较，包括前缀长度、前缀初始化、只调整嵌入层、在输入中插入可训练的激活等。结果表明，完全的前缀调优比其他变体更具表达力和稳定性。


这篇论文的第二部分讲述了：

- **低数据和外推设置下的性能**：在低数据和外推设置下，前缀调优优于微调和适配器调优，表明前缀调优能够更好地利用预训练语言模型的泛化能力。
- **前缀调优和适配器调优的区别**：前缀调优和适配器调优都冻结了预训练参数，但它们通过不同的方式影响变换器的激活层。前缀调优使用前缀和预训练注意力块来影响后续的激活，而适配器调优在每层之间插入可训练模块来直接添加残差向量。
- **前缀调优的参数效率**：前缀调优比适配器调优需要更少的参数，同时保持了可比的性能。这可能是因为前缀调优尽可能地保持了预训练语言模型的完整性，因此比适配器调优更多地利用了语言模型。
- **相关工作**：介绍了一些与本文相关的工作，包括轻量级微调、提示、可控制生成等，并指出它们与本文的异同点。


# 1. 这篇论文的主要贡献是什么？

- **前缀调整**：一种用于生成任务的轻量级微调方法，它保持预训练语言模型的参数不变，但优化一个小的任务特定的向量（称为前缀）。
- **前缀调整的优势**：与微调相比，前缀调整可以节省存储空间，提高模块化，适应低数据设置，并更好地推广到未见过的主题。
- **实验结果**：在表格到文本和摘要两个任务上，前缀调整使用GPT-2和BART作为基础模型，与微调和其他轻量级微调方法进行了比较。结果表明，前缀调整在保持或提高生成质量的同时，显著减少了任务特定的参数。



# 2. 这个贡献重要吗？为什么？
这篇文章的贡献是重要的，因为它提出了一种新的轻量级微调方法，叫做前缀调整，它可以用于自然语言生成任务。这种方法有以下几个优点：

- **节省存储空间**：前缀调整只需要优化一个小的任务特定的向量（称为前缀），而不需要修改预训练语言模型的参数。因此，它只需要存储一个大的语言模型和一个小的前缀，而不需要为每个任务存储一个完整的模型副本。
- **提高模块化**：前缀调整可以将预训练语言模型和任务特定的前缀分开，从而实现模块化。这样，一个语言模型就可以支持多个任务，而不需要重新训练或微调。这对于个性化或多任务学习等场景是有益的。
- **适应低数据设置**：前缀调整在低数据设置下可以优于微调，因为它可以保留预训练语言模型的优势，而不会过拟合少量的数据。实验结果表明，在表格到文本和摘要两个任务上，前缀调整在数据量较小时都比微调表现更好。
- **更好地推广到未见过的主题**：前缀调整在推广到未见过的主题方面也比微调更好，因为它不会破坏预训练语言模型的泛化能力。实验结果表明，在WebNLG和XSUM两个数据集上，前缀调整在测试集中包含训练集中没有出现过的主题时都比微调得到更高的评分。

这些优点使得前缀调整是一种有效且创新的方法，它可以利用大型预训练语言模型来解决自然语言生成任务，同时节省存储空间和计算资源。

# 3. 这篇论文的局限是什么？

- **适用范围**：前缀调整只适用于自然语言生成任务，而不适用于自然语言理解任务，如文本分类和实体识别。
- **模型选择**：前缀调整依赖于预训练语言模型的质量和泛化能力，因此在选择模型时需要考虑模型的大小、结构和预训练目标。
- **前缀长度**：前缀调整需要确定合适的前缀长度，过长或过短的前缀都会影响生成质量和效率。
- **初始化策略**：前缀调整需要选择合适的初始化策略，以避免在低数据设置下出现不稳定或过拟合的情况。

# 4. 根据这篇文章的结果，你得到什么启发？

- **利用预训练语言模型的潜力**：预训练语言模型已经具有强大的泛化能力和表达能力，通过添加一个小的前缀，就可以实现不同任务的自然语言生成，而无需修改语言模型的参数。
- **探索轻量级微调方法的可能性**：前缀调整是一种新颖且有效的轻量级微调方法，它可以用于自然语言生成任务，同时节省存储空间和计算资源。它也为其他轻量级微调方法提供了灵感和参考。
- **平衡参数数量和生成质量的关系**：前缀调整表明，在一定范围内，增加前缀长度可以提高生成质量，但超过一定阈值后会导致过拟合或性能下降。因此，在设计轻量级微调方法时，需要考虑参数数量和生成质量之间的平衡。
- **前缀调整和适应器调整的比较**：这两种方法都是轻量级微调的方法，它们都保持预训练语言模型的参数不变，但是通过不同的方式影响Transformer的激活层。前缀调整使用一个小的任务特定的向量（前缀），而适应器调整在语言模型的层之间插入可训练的模块。
- **前缀调整的参数效率**：前缀调整相比适应器调整需要大大减少的参数数量，同时保持了可比较的性能。这可能是因为前缀调整尽可能地保持了预训练语言模型的完整性，从而更好地利用了语言模型。
- **低维重参数化的解释**：同时工作的一篇论文使用内在维度来证明，存在一个低维重参数化，它对于微调来说和全参数空间一样有效。这解释了为什么只更新少量参数就可以获得良好的下游任务准确率。我们的工作也呼应了这一发现，表明只更新一个很小的前缀就可以获得良好的生成性能。
- **结论**：这篇论文提出了前缀调整，一种用于自然语言生成任务的轻量级微调方法，它在预训练语言模型前面添加一个可训练的连续前缀。我们发现，尽管学习1000倍少于微调的参数，前缀调整在全数据设置下可以保持可比较的性能，在低数据和外推设置下优于微调。

# 5. 这篇论文的研究假设是什么？这些假设是否合理、局限或过于简化？

这篇论文的研究假设有以下几点：

- **预训练语言模型的潜力**：预训练语言模型已经具有强大的泛化能力和表达能力，通过添加一个小的前缀，就可以实现不同任务的自然语言生成，而无需修改语言模型的参数。
- **前缀调整的优势**：与微调相比，前缀调整可以节省存储空间，提高模块化，适应低数据设置，并更好地推广到未见过的主题。
- **前缀调整的可行性**：通过优化一个小的任务特定的向量（称为前缀），可以有效地影响预训练语言模型的激活层和后续生成的概率分布。

这些假设是基于对预训练语言模型和自然语言生成任务的理解和分析而提出的，它们在一定程度上是合理和有效的。但是，它们也有一些局限或过于简化的地方，例如：

- **适用范围**：前缀调整只适用于自然语言生成任务，而不适用于自然语言理解任务，如文本分类和实体识别。
- **模型选择**：前缀调整依赖于预训练语言模型的质量和泛化能力，因此在选择模型时需要考虑模型的大小、结构和预训练目标。
- **前缀长度**：前缀调整需要确定合适的前缀长度，过长或过短的前缀都会影响生成质量和效率。
- **初始化策略**：前缀调整需要选择合适的初始化策略，以避免在低数据设置下出现不稳定或过拟合的情况。


# 6. 基于这篇论文的可能应用有哪些？

- **轻量级微调**：前缀调整是一种用于自然语言生成任务的轻量级微调方法，它可以利用大型预训练语言模型，同时节省存储空间和计算资源。这对于需要部署多个任务的NLP系统或个性化的NLP服务是有益的。
- **自然语言生成**：前缀调整可以用于各种自然语言生成任务，如表格到文本、摘要、机器翻译、对话生成等。它可以保持或提高生成质量，并在低数据和外推设置下优于微调。
- **可控制的生成**：前缀调整可以通过优化一个小的任务特定的向量（前缀）来控制预训练语言模型的生成。这可以用于实现不同的生成目标，如风格、情感、主题等。

# 7. 在该文基础上，有些工作可以继续延伸下去？如何延伸？

- **探索其他轻量级微调方法**：除了前缀调整，还可以尝试其他方式来影响预训练语言模型的激活层，例如后缀调整、中缀调整、或者使用多个前缀来控制不同层次的生成。
- **应用到其他自然语言生成任务**：除了表格到文本和摘要两个任务外，还可以将前缀调整应用到其他自然语言生成任务，如机器翻译、对话生成、文本复述等，并比较其与微调和其他轻量级微调方法的性能差异。
- **结合离散提示和连续提示**：除了使用连续向量作为前缀外，还可以尝试使用离散单词或符号作为提示，并与连续向量结合起来进行优化。这样可能可以提高提示的可解释性和可控制性，并且利用预训练语言模型中已有的知识。
- **分析前缀向量的含义和作用**：除了评估生成质量外，还可以对前缀向量进行深入的分析和可视化，以探究它们是如何影响预训练语言模型的激活层和生成概率分布的。这样可能可以揭示前缀向量所隐含的语义或结构信息，并且提高对生成过程的理解。


# 8. 这篇论文中，哪些是你还没明白的地方？

# 9. 这篇论文与你以前阅读过的论文有何关系？

- **轻量级微调**：一些研究探索了如何通过冻结大部分预训练参数，并在预训练模型中插入小型可训练模块来实现轻量级微调。例如，适配器调优（adapter-tuning）在每层预训练语言模型之间插入任务特定层（Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020）。
- **提示**：提示是指在任务输入前加上自然语言任务说明和一些例子，并从语言模型中生成输出。GPT-3 (Brown et al., 2020)使用人工设计的提示来适应不同的任务，这种框架被称为上下文学习（in-context learning）。¹[1]Shin et al. (2020)提出了自动提示（AutoPrompt），通过搜索一系列离散的触发词并与每个输入连接起来，来从掩码语言模型中引出情感或事实知识。
- **可控制生成**：可控制生成旨在引导预训练语言模型匹配句子级属性（例如正面情感或体育话题）。例如，Keskar et al. (2019)预训练了一个语言模型（CTRL），使其根据元数据（如关键词或网址）进行条件生成；Dathathri et al. (2020)提出了一个插件式语言模型（PPLM），通过迭代更新过去的激活来影响下一个令牌分布。

这些论文与本文的关系：

- **轻量级微调**与本文的方法有共同之处，都是在保持预训练参数不变的同时，增加少量的任务特定参数。但是，本文的方法更加轻量级，只需要0.1%的任务特定参数，而轻量级微调需要2-4%的任务特定参数。
- **提示**与本文的灵感有关联，都是利用输入序列的前缀来引导语言模型的生成。但是，本文的前缀是连续向量，而不是离散令牌，因此更具表达力和灵活性。而且，本文的前缀是通过优化学习得到的，而不是人工设计或搜索得到的。
- **可控制生成**与本文的目标有相似之处，都是为了使预训练语言模型能够完成不同的自然语言生成任务。但是，本文的方法更加通用和简单，只需要优化一个小型的前缀向量，而不需要额外的元数据或激活更新。



# 10. 基于这篇论文的思想，若要做一个项目，能否用两句话描述一下这个项目的大致思想？

- 项目的目的是使用一个小型的可训练的前缀来调整预训练的语言模型，使其能够完成不同的自然语言生成任务，例如表格到文本生成和摘要生成。
- 项目的方法是在输入序列的开头添加一个连续向量序列作为前缀，这个前缀是由一个可训练的矩阵和一个多层感知机组成的。前缀可以影响语言模型对输入的编码和对输出的生成，而不改变语言模型的参数。




# 疑难思考

## 前缀调整是如何工作的？
前缀调整是一种轻量级微调方法，它通过优化一个小的任务特定的向量（称为前缀）来生成任务，同时保持预训练语言模型的参数不变。这种方法的工作原理是通过将前缀与预训练语言模型的隐藏状态相结合，然后将其输入到解码器中以生成文本。前缀可以被视为一种条件，它可以指导预训练语言模型生成特定任务的文本。由于前缀的大小远远小于预训练语言模型的参数数量，因此前缀调整可以节省存储空间，并且可以更快地进行微调。

## 有哪些其他轻量级微调方法？
- **适应性输入**：这种方法通过在预训练语言模型的输入端添加一个小的线性层来进行微调。
- **适应性输出**：这种方法通过在预训练语言模型的输出端添加一个小的线性层来进行微调。
- **适应性输入和输出**：这种方法结合了适应性输入和适应性输出，通过在预训练语言模型的输入和输出端都添加一个小的线性层来进行微调。
- **动态评估**：这种方法通过在解码过程中使用一个小的神经网络来动态调整预训练语言模型的输出概率分布来进行微调。

## 动态评估是如何工作的？
动态评估是一种轻量级微调方法，它通过在解码过程中使用一个小的神经网络来动态调整预训练语言模型的输出概率分布来进行微调。这种方法的工作原理是通过在解码过程中，根据当前的上下文和已生成的文本，使用一个小的神经网络来动态调整预训练语言模型的输出概率分布。这样，预训练语言模型可以更好地适应特定任务，并生成更符合任务要求的文本。由于动态评估只需要优化一个小的神经网络，因此它可以快速地进行微调，并且可以节省存储空间。


## GPT-2和BART之间有什么区别？
GPT-2和BART是两种不同的预训练语言模型，它们都基于Transformer架构，但在许多方面有所不同。

- **模型结构**：GPT-2是一个自回归语言模型，它使用了Transformer的解码器部分。而BART是一个序列到序列模型，它使用了Transformer的编码器和解码器部分。
- **预训练目标**：GPT-2的预训练目标是通过给定前文来预测下一个单词。而BART的预训练目标是通过对输入序列进行噪声转换，然后重建原始序列。
- **应用范围**：由于GPT-2是一个自回归语言模型，它更适用于生成任务，如文本生成和语言翻译。而BART作为一个序列到序列模型，它不仅可以用于生成任务，还可以用于自然语言理解任务，如文本分类和摘要。

## BART和T5之间有什么区别？
BART和T5都是基于Transformer架构的预训练语言模型，它们都采用了Transformers原始结构，但在许多方面有所不同。

- **预训练任务**：BART和T5的预训练任务不太一样。BART的预训练任务是Text Infilling+Sentence permutation，其中Text Infilling起到了最主要的作用，其实就是Span级别的mask。而T5分为无监督和有监督，其中无监督任务也是Span level的mask。
- **位置编码**：BART和T5在位置编码上也有所不同。BART采用可学习的绝对位置嵌入，而T5改成了相对位置嵌入。
- **激活函数**：BART和T5在激活函数上也有所不同。BART的激活函数是GELU²，而T5采用Relu激活函数。

## 数据集介绍

- **表格到文本生成**：E2E，WebNLG，和DART。这些数据集包含了不同的领域和复杂度的数据表格和对应的文本描述。
- **摘要生成**：XSUM。这个数据集包含了新闻文章和对应的摘要。




