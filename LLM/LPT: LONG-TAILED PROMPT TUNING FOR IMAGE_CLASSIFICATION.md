![image](https://github.com/leejamesss/paper-reading/assets/117844938/ce3df1b6-535f-4d6d-b462-b4445251d0d4)
![image](https://github.com/leejamesss/paper-reading/assets/117844938/f42ee51b-94f5-46a8-b598-a47f07c4917d)
![image](https://github.com/leejamesss/paper-reading/assets/117844938/85415c0e-544d-40fa-b10b-84e05561b363)
![image](https://github.com/leejamesss/paper-reading/assets/117844938/ce3df1b6-535f-4d6d-b462-b4445251d0d4)

  
# 1. 这篇论文的主要贡献是什么？

- 提出了一种有效的长尾提示调整（LPT）方法，用于长尾分类任务。LPT在一个冻结的预训练模型中引入了可训练的提示，以适应长尾数据。LPT包括两个阶段：共享提示调整和组提示调整。
- 设计了两种类型的提示：1）共享提示，用于学习所有类别的通用特征，并将预训练模型适应到目标长尾域；2）组特定提示，用于收集具有相似特征的样本的组特定特征，并赋予预训练模型细粒度的判别能力。
- 设计了一个两阶段的训练范式来学习这些提示。在第一阶段，LPT通过常规的监督提示调整来训练共享提示，以将预训练模型适应到期望的长尾域。在第二阶段，LPT使用学习到的共享提示作为查询，从组特定提示集合中选择一个小的最佳匹配集合，以挖掘这些相似样本的共同特征，并使用双重采样策略和非对称高斯云对数损失来优化这些提示。
- 通过仅微调少量的提示而固定预训练模型，LPT可以降低训练成本和部署成本，同时享受预训练模型的强大泛化能力。实验表明，在各种长尾基准数据集上，LPT仅增加了约1.1%的额外可训练参数，就实现了与先前整个模型微调方法相当或更高的性能，并且对域移动更加鲁棒。

# 2. 这个贡献重要吗？为什么？



# 3. 这篇论文的局限是什么？

# 4. 根据这篇文章的结果，你得到什么启发？

# 5. 这篇论文的研究假设是什么？这些假设是否合理、局限或过于简化？

# 6. 基于这篇论文的可能应用有哪些？

# 7. 在该文基础上，有些工作可以继续延伸下去？如何延伸？

# 8. 这篇论文中，哪些是你还没明白的地方？

# 9. 还有什么其他相关的论文？它们之间有什么关系？

# 10. 基于这篇论文的思想，若要做一个项目，能否用两句话描述一下这个项目的大致思想？
